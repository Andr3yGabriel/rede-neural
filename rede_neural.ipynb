{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f93d4fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "778f49db",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.ToTensor()\n",
    "\n",
    "trainset = datasets.MNIST('./data', download=True, train=True, transform=transforms)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "valset = datasets.MNIST('./data', download=True, train=False, transform=transforms)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7e62953d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a881c846e0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, etiquetas = next(dataiter)\n",
    "plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6a3602e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(images[0].shape) #Verificando as dimensões do tensor de cada imagem\n",
    "print(etiquetas[0].shape) #Verificando as dimensões do tensor de cada etiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f6d471d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modelo(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Modelo, self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, 128) #Camada de entrada, 748 neurônios se ligam a 128\n",
    "        self.linear2 = nn.Linear(128, 64) #Camada interna 1, 128 neurônios se ligam a 64\n",
    "        self.linear3 = nn.Linear(64, 10) #Camada interna 2, 64 neurônios se ligam a 10\n",
    "        # para a camada de saida não é necessário definir nada pois só precisamos pegar o output da camada interna 2\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.linear1(X)) # função de ativação da camada de entrada para a camada interna 1\n",
    "        X = F.relu(self.linear2(X)) # função de ativação da camada interna 1 para a camada interna 2\n",
    "        X = self.linear3(X) # função de ativação da camada interna 2 para a camada de saída, nesse caso f(x) = x\n",
    "        return F.log_softmax(X, dim=1) # dados utilizados para calcular a perda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "80a218ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treino(modelo, trainloader, device):\n",
    "    otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.9) # define a política de atualização dos pesos e da bias\n",
    "    inicio = time() # inicia o cronômetro\n",
    "    \n",
    "    criterio = nn.NLLLoss() # define o criterio para calcular a perda\n",
    "    EPOCHS = 10 # numero de epochs que o algoritmo rodará\n",
    "    modelo.train() # coloca o modelo em modo de treinamento\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        perda_acumulada = 0 # inicialização da perda acumulada da epoch em questão\n",
    "        \n",
    "        for imagens, etiquetas in trainloader:\n",
    "            imagens = imagens.view(imagens.shape[0], -1) # convertendo as imagens para vetores de 28*28 casas para ficarem compativeis com a rede neural\n",
    "            otimizador.zero_grad() # zerando os gradientes por conta do ciclo anterior\n",
    "            \n",
    "            output = modelo(imagens.to(device)) # colocando os dados no modelo\n",
    "            perda_instantanea = criterio(output, etiquetas.to(device)) # calculando a perda da epoch em questão\n",
    "            \n",
    "            perda_instantanea.backward() # back propagation a partir da perda\n",
    "            \n",
    "            otimizador.step() # atualizando os pesos e a bias\n",
    "            \n",
    "            perda_acumulada += perda_instantanea.item() # atualização da perda acumulada\n",
    "            \n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} - Perda: {perda_acumulada/len(trainloader)}\")\n",
    "    print(\"\\nTempo de treino: \", time()-inicio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1ad9fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validacao(modelo, valloader, device):\n",
    "    conta_corretas, conta_total, conta_incorretas = 0, 0, 0\n",
    "    for imagens, etiquetas in valloader:\n",
    "        for i in range(len(etiquetas)):\n",
    "            img = imagens[i].view(1, 784) \n",
    "            # desativar o autograd para acelerar a validação. Grafos computacionais dinâmicos tem um custo alto de processamento\n",
    "            with torch.no_grad():\n",
    "                logps = modelo(img.to(device)) # output do modelo em escala logaritmica\n",
    "                \n",
    "            ps = torch.exp(logps) # convertendo o output do modelo para a escala normal (lembrando que é um tensor)\n",
    "            probab = list(ps.cpu().numpy()[0])\n",
    "            etiqueta_pred = probab.index(max(probab)) # converte o tensor em um número, no caso, o número que o modelo previu como correto\n",
    "            etiqueta_certa = etiquetas.numpy()[i]\n",
    "            \n",
    "            if etiqueta_certa != etiqueta_pred:\n",
    "                conta_incorretas += 1\n",
    "            \n",
    "            if (etiqueta_certa == etiqueta_pred):\n",
    "                conta_corretas += 1\n",
    "            conta_total += 1\n",
    "            \n",
    "    print(\"Total de imagens testadas = \", conta_total)\n",
    "    print(\"\\nPrecisão do modelo = {}%\".format(100*conta_corretas/conta_total))\n",
    "    print(\"Total de imagens incorretas = \", conta_incorretas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1c1306e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Perda: 0.037833731997061144\n",
      "\n",
      "Tempo de treino:  63.84479784965515\n",
      "Total de imagens testadas =  10000\n",
      "\n",
      "Precisão do modelo = 97.8%\n",
      "Total de imagens incorretas =  220\n"
     ]
    }
   ],
   "source": [
    "modelo = Modelo()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # verifica se a GPU está disponível e define o dispositivo\n",
    "modelo.to(device) # coloca o modelo no dispositivo definido acima\n",
    "\n",
    "treino(modelo, trainloader, device) # treina o modelo\n",
    "validacao(modelo, valloader, device) # valida o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d90948",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
